{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-20c545302f05>:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATAPATH)\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = 'data/movies_metadata.csv'\n",
    "\n",
    "df = pd.read_csv(DATAPATH)\n",
    "df = df[~df.overview.isna()]\n",
    "df.rename(columns={'overview': 'sentence'}, inplace=True)\n",
    "df = df.iloc[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TF-IDF Modeling\n",
    "\n",
    "# Embedded Matrices - 100 movie description sentences and embed vector size = 300\n",
    "# then the matrix size is [100, 300]\n",
    "\n",
    "# When user inputs a sentence, we embed its query sentence into a 300-dim vector\n",
    "# with the same model and compute cos distance between the query vector and the 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning sentences...\n"
     ]
    }
   ],
   "source": [
    "# We first need to clean the sentence of un-needed words, numbers, dirty and alphanumeric\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 4\n",
    "MAX_WORDS = 200\n",
    "\n",
    "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace \n",
    "PATTERN_NUM = re.compile(r'[0-9]')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Series of cleaning numbers, lower cases and removing stopwords\n",
    "    return text (modified string)\n",
    "    \"\"\"\n",
    "    if text: # ensures we have text\n",
    "        text = re.sub(PATTERN_NUM, '', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(PATTERN_S, ' ', text)\n",
    "        text = re.sub(PATTERN_RN, ' ', text)\n",
    "        text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "        return text\n",
    "\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words\n",
    "    \"\"\"\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    tokens = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                   and w not in stopwords)]\n",
    "    return tokens\n",
    "    \n",
    "def clean_sentences(df):\n",
    "    \"\"\"\n",
    "    Remove irrelavant chars in new column clean_sentence\n",
    "    Lemmatize, tokenize words into list of words in column tok_lem_sentence\n",
    "    \"\"\"\n",
    "    print(\"Cleaning sentences...\")\n",
    "    df['clean_sentence'] = df['sentence'].apply(clean_text)\n",
    "    df['tok_lem_sentence'] = df['clean_sentence'].apply(\n",
    "        lambda x: tokenizer(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True))\n",
    "    return df\n",
    "\n",
    "df = clean_sentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "      <th>tok_lem_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>led by woody  andy  toys live happily in his r...</td>\n",
       "      <td>[woody, happily, birthday, brings, lightyear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>when siblings judy and peter discover an encha...</td>\n",
       "      <td>[sibling, peter, discover, enchanted, board, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>a family wedding reignites the ancient feud be...</td>\n",
       "      <td>[family, wedding, reignites, ancient, neighbor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>cheated on  mistreated and stepped on  the wom...</td>\n",
       "      <td>[cheated, mistreated, stepped, woman, holding,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>just when george banks has recovered from his ...</td>\n",
       "      <td>[george, recovered, daughter, wedding, receive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20131</th>\n",
       "      <td>After a lifetime of hiding, Chely Wright becom...</td>\n",
       "      <td>after a lifetime of hiding  chely wright becom...</td>\n",
       "      <td>[lifetime, hiding, chely, wright, becomes, fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20132</th>\n",
       "      <td>In 1989, five black and Latino teenagers from ...</td>\n",
       "      <td>in   five black and latino teenagers from harl...</td>\n",
       "      <td>[black, latino, teenager, harlem, arrested, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20133</th>\n",
       "      <td>Arkin escapes with his life from the vicious g...</td>\n",
       "      <td>arkin escapes with his life from the vicious g...</td>\n",
       "      <td>[arkin, escape, vicious, collector, entrapment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20134</th>\n",
       "      <td>Remake of a hit film from 1990, \"The Cherry Or...</td>\n",
       "      <td>remake of a hit film from    the cherry orchar...</td>\n",
       "      <td>[remake, cherry, orchard, nakahara, directed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20135</th>\n",
       "      <td>Chronicles the adventures of Franklin Delano R...</td>\n",
       "      <td>chronicles the adventures of franklin delano r...</td>\n",
       "      <td>[chronicle, adventure, franklin, delano, roose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      Led by Woody, Andy's toys live happily in his ...   \n",
       "1      When siblings Judy and Peter discover an encha...   \n",
       "2      A family wedding reignites the ancient feud be...   \n",
       "3      Cheated on, mistreated and stepped on, the wom...   \n",
       "4      Just when George Banks has recovered from his ...   \n",
       "...                                                  ...   \n",
       "20131  After a lifetime of hiding, Chely Wright becom...   \n",
       "20132  In 1989, five black and Latino teenagers from ...   \n",
       "20133  Arkin escapes with his life from the vicious g...   \n",
       "20134  Remake of a hit film from 1990, \"The Cherry Or...   \n",
       "20135  Chronicles the adventures of Franklin Delano R...   \n",
       "\n",
       "                                          clean_sentence  \\\n",
       "0      led by woody  andy  toys live happily in his r...   \n",
       "1      when siblings judy and peter discover an encha...   \n",
       "2      a family wedding reignites the ancient feud be...   \n",
       "3      cheated on  mistreated and stepped on  the wom...   \n",
       "4      just when george banks has recovered from his ...   \n",
       "...                                                  ...   \n",
       "20131  after a lifetime of hiding  chely wright becom...   \n",
       "20132  in   five black and latino teenagers from harl...   \n",
       "20133  arkin escapes with his life from the vicious g...   \n",
       "20134  remake of a hit film from    the cherry orchar...   \n",
       "20135  chronicles the adventures of franklin delano r...   \n",
       "\n",
       "                                        tok_lem_sentence  \n",
       "0      [woody, happily, birthday, brings, lightyear, ...  \n",
       "1      [sibling, peter, discover, enchanted, board, m...  \n",
       "2      [family, wedding, reignites, ancient, neighbor...  \n",
       "3      [cheated, mistreated, stepped, woman, holding,...  \n",
       "4      [george, recovered, daughter, wedding, receive...  \n",
       "...                                                  ...  \n",
       "20131  [lifetime, hiding, chely, wright, becomes, fir...  \n",
       "20132  [black, latino, teenager, harlem, arrested, la...  \n",
       "20133  [arkin, escape, vicious, collector, entrapment...  \n",
       "20134  [remake, cherry, orchard, nakahara, directed, ...  \n",
       "20135  [chronicle, adventure, franklin, delano, roose...  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df[['sentence', 'clean_sentence', 'tok_lem_sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query sentence to compare cos distances against\n",
    "query_sentence = 'a crime story with a beautiful woman'\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract best indices using cos distance\n",
    "def extract_best_indices(mat, topk, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cos distance over all tokens.\n",
    "    m (Numpy 2D array): cos distance matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to low in order)\n",
    "    \"\"\"\n",
    "    print(\"--------Extract best indices:--------\")\n",
    "    print(\"mat.shape:\")\n",
    "    print(mat.shape)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #return the sum on all tokens of cosines for each sentence\n",
    "    if len(mat.shape) > 1:\n",
    "        cos_sim = np.mean(mat, axis=0)\n",
    "    else:\n",
    "        cos_sim = mat\n",
    "        \n",
    "    # sort indices from high to low\n",
    "    index = np.argsort(cos_sim)[::-1]\n",
    "    if mask is not None:\n",
    "        assert mask.shape == mat.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "        \n",
    "    # rm all 0 cosine distance\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) # elim 0 cos dist\n",
    "    best_index = index[mask][:topk]\n",
    "    return best_index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 44327)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainmasking the TF-IDF Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# adapt stop words\n",
    "token_stop = tokenizer(' '.join(STOPWORDS), lemmatize=False)\n",
    "\n",
    "# Fit the TFIDF - fit_transform() calculates the IDF and return doc_term matrix\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
    "tfidf_mat = vectorizer.fit_transform(df['sentence'].values) # -> (num_sentences, num_vocabulary)\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens size and value:\n",
      "4\n",
      "['crime', 'story', 'beautiful', 'woman']\n",
      "\n",
      "\n",
      "Vectorizer transform from input sentence:\n",
      "(4, 44327)\n",
      "  (0, 9461)\t1.0\n",
      "  (1, 37981)\t1.0\n",
      "  (2, 4187)\t1.0\n",
      "  (3, 43611)\t1.0\n",
      "\n",
      "\n",
      "Similarity matrix:\n",
      "(4, 20000)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.06682396 0.        ]\n",
      " [0.         0.         0.         ... 0.08708068 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "\n",
      "--------Extract best indices:--------\n",
      "mat.shape:\n",
      "(4, 20000)\n",
      "\n",
      "\n",
      "best_index:\n",
      "[14349  8977 10463]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14402</th>\n",
       "      <td>Michael Jackson: Life of a Superstar</td>\n",
       "      <td>[{'id': 99, 'name': 'Documentary'}, {'id': 10770, 'name': 'TV Movie'}]</td>\n",
       "      <td>The Story of the King of Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>Innocent Blood</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 27, 'name': 'Horror'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]</td>\n",
       "      <td>A beautiful vampire turns a crime lord into a creature of the night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10493</th>\n",
       "      <td>Requiem pour un vampire</td>\n",
       "      <td>[{'id': 27, 'name': 'Horror'}]</td>\n",
       "      <td>A vampire lures beautiful young women to his castle in Europe.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             original_title  \\\n",
       "14402  Michael Jackson: Life of a Superstar   \n",
       "9003                         Innocent Blood   \n",
       "10493               Requiem pour un vampire   \n",
       "\n",
       "                                                                                                                          genres  \\\n",
       "14402                                                     [{'id': 99, 'name': 'Documentary'}, {'id': 10770, 'name': 'TV Movie'}]   \n",
       "9003   [{'id': 35, 'name': 'Comedy'}, {'id': 27, 'name': 'Horror'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]   \n",
       "10493                                                                                             [{'id': 27, 'name': 'Horror'}]   \n",
       "\n",
       "                                                                   sentence  \n",
       "14402                                          The Story of the King of Pop  \n",
       "9003   A beautiful vampire turns a crime lord into a creature of the night.  \n",
       "10493        A vampire lures beautiful young women to his castle in Europe.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run predictions on the query sentence\n",
    "def get_recommendations_tfidf(sentence, tfidf_mat, topn):\n",
    "    tokens = [str(tok) for tok in tokenizer(sentence)]\n",
    "    print(\"Tokens size and value:\")\n",
    "    print(len(tokens))\n",
    "    print(tokens)\n",
    "    print(\"\\n\")\n",
    "    vec = vectorizer.transform(tokens)\n",
    "    print(\"Vectorizer transform from input sentence:\")\n",
    "    print(vec.shape)\n",
    "    print(vec)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # create similarity list between the query and the dataset\n",
    "    mat = cosine_similarity(vec, tfidf_mat)\n",
    "    print(\"Similarity matrix:\")\n",
    "    print(mat.shape)\n",
    "    print(mat)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # extract the top 3 indices (ie rows giving the best cosine dist)\n",
    "    best_idx = extract_best_indices(mat, topk=topn)\n",
    "    return best_idx\n",
    "\n",
    "# get the topn = 3 recommendations \n",
    "best_index = get_recommendations_tfidf(query_sentence, tfidf_mat, 3)\n",
    "print(\"best_index:\")\n",
    "print(best_index)\n",
    "print(\"\\n\")\n",
    "\n",
    "# get the values from the DF for those indices\n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning SPACY Modeling'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Learning SPACY Modeling'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load the pretrained en_core_web_lg model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# apply the model to the input sentence\n",
    "df[\"spacy_sentence\"] = df['sentence'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                                                                                       (Led, by, Woody, ,, Andy, 's, toys, live, happily, in, his, room, until, Andy, 's, birthday, brings, Buzz, Lightyear, onto, the, scene, ., Afraid, of, losing, his, place, in, Andy, 's, heart, ,, Woody, plots, against, Buzz, ., But, when, circumstances, separate, Buzz, and, Woody, from, their, owner, ,, the, duo, eventually, learns, to, put, aside, their, differences, .)\n",
      "1                              (When, siblings, Judy, and, Peter, discover, an, enchanted, board, game, that, opens, the, door, to, a, magical, world, ,, they, unwittingly, invite, Alan, --, an, adult, who, 's, been, trapped, inside, the, game, for, 26, years, --, into, their, living, room, ., Alan, 's, only, hope, for, freedom, is, to, finish, the, game, ,, which, proves, risky, as, all, three, find, themselves, running, from, giant, rhinoceroses, ,, evil, monkeys, and, other, terrifying, creatures, .)\n",
      "2                                                                                                         (A, family, wedding, reignites, the, ancient, feud, between, next, -, door, neighbors, and, fishing, buddies, John, and, Max, ., Meanwhile, ,, a, sultry, Italian, divorcée, opens, a, restaurant, at, the, local, bait, shop, ,, alarming, the, locals, who, worry, she, 'll, scare, the, fish, away, ., But, she, 's, less, interested, in, seafood, than, she, is, in, cooking, up, a, hot, time, with, Max, .)\n",
      "3                                                                                                                                                                   (Cheated, on, ,, mistreated, and, stepped, on, ,, the, women, are, holding, their, breath, ,, waiting, for, the, elusive, \", good, man, \", to, break, a, string, of, less, -, than, -, stellar, lovers, ., Friends, and, confidants, Vannah, ,, Bernie, ,, Glo, and, Robin, talk, it, all, out, ,, determined, to, find, a, better, way, to, breathe, .)\n",
      "4                                                                                                             (Just, when, George, Banks, has, recovered, from, his, daughter, 's, wedding, ,, he, receives, the, news, that, she, 's, pregnant, ..., and, that, George, 's, wife, ,, Nina, ,, is, expecting, too, ., He, was, planning, on, selling, their, home, ,, but, that, 's, a, plan, that, --, like, George, --, will, have, to, change, with, the, arrival, of, both, a, grandchild, and, a, kid, of, his, own, .)\n",
      "                                                                                                                                                                                                                                                                ...                                                                                                                                                                                                                                                         \n",
      "20131    (After, a, lifetime, of, hiding, ,, Chely, Wright, becomes, the, first, commercial, country, music, singer, to, come, out, as, gay, ,, shattering, cultural, stereotypes, within, Nashville, ,, per, conservative, heartland, family, and, ,, most, importantly, ,, within, herself, ., With, unprecedented, access, over, a, two, -, year, period, ,, including, her, private, video, diaries, ,, the, film, layers, Chely, 's, rise, to, fame, while, hiding, in, the, late, 90, 's, with, the, execution, of,...\n",
      "20132    (In, 1989, ,, five, black, and, Latino, teenagers, from, Harlem, were, arrested, and, later, convicted, of, raping, a, white, woman, in, New, York, City, 's, Central, Park, ., They, spent, between, 6, and, 13, years, in, prison, before, a, serial, rapist, confessed, that, he, alone, had, committed, the, crime, ,, leading, to, their, convictions, being, overturned, ., Set, against, a, backdrop, of, a, decaying, city, beset, by, violence, and, racial, tension, ,, this, is, the, story, of, that...\n",
      "20133               (Arkin, escapes, with, his, life, from, the, vicious, grips, of, \", The, Collector, \", during, an, entrapment, party, where, he, adds, beautiful, Elena, to, his, \", Collection, ., \", Instead, of, recovering, from, the, trauma, ,, Arkin, is, suddenly, abducted, from, the, hospital, by, mercenaries, hired, by, Elena, 's, wealthy, father, ., Arkin, is, blackmailed, to, team, up, with, the, mercenaries, and, track, down, The, Collector, 's, booby, trapped, warehouse, and, save, Elena, .)\n",
      "20134    (Remake, of, a, hit, film, from, 1990, ,, \", The, Cherry, Orchard, \", which, Nakahara, himself, directed, ,, based, on, a, popular, comic, book, from, Akimi, Yoshida, ., The, story, takes, place, at, a, prestigious, girls, high, school, where, everyone, is, oppressed, with, rules, ., One, girl, who, just, transferred, to, this, school, finds, a, long, lost, script, of, Anton, Chekhov, 's, \", The, Cherry, Orchard, \", which, was, banned, from, school, 11, years, ago, ., Breaking, strict, schoo...\n",
      "20135                                                                                                                                                                                               (Chronicles, the, adventures, of, Franklin, Delano, Roosevelt, ,, as, he, rides, a, \", wheelchair, of, death, \", to, stop, the, world, from, being, taking, over, by, polio, -, carrying, werewolves, during, WWII, ., A, deadly, menace, that, are, led, by, Werewolf, Hitler, ,, Mussolini, and, Emperor, Hirohito, .)\n",
      "Name: spacy_sentence, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"spacy_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: How does spacy compare against the TF-IDF model??\n",
    "# Answer 1: Takes like 10 times as long!\n",
    "# check python processes\n",
    "import psutil\n",
    "\n",
    "def check_process_status(process_name):\n",
    "    \"\"\"\n",
    "    Return status of process based on process name.\n",
    "    \"\"\"\n",
    "    process_status = [ proc for proc in psutil.process_iter() if proc.name() == process_name ]\n",
    "    if process_status:\n",
    "        for current_process in process_status:\n",
    "            print(\"Process id is %s, name is %s, staus is %s\"%(current_process.pid, current_process.name(), current_process.status()))\n",
    "    else:\n",
    "        print(\"Process name not valid\", process_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "Widowed U.S. president Andrew Shepherd, one of the world's most powerful men, can have anything he wants -- and what he covets most is Sydney Ellen Wade, a Washington lobbyist. But Shepherd's attempts at courting her spark wild rumors and decimate his approval ratings.\n"
     ]
    }
   ],
   "source": [
    "# retrieve embedded vectors as a matrix\n",
    "embed_mat = df['spacy_sentence'].values\n",
    "print(embed_mat.shape)\n",
    "print(embed_mat[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query sentence = a crime story with a beautiful woman\n",
      "--------Extract best indices:--------\n",
      "mat.shape:\n",
      "(20000,)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-760bf85b960a>:12: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  mat = np.array([query_embed.similarity(line) for line in embed_mat])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15538</th>\n",
       "      <td>In a Day</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]</td>\n",
       "      <td>A young woman spends a curiously unpredictable day with a stranger.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6120</th>\n",
       "      <td>Les dames du Bois de Boulogne</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]</td>\n",
       "      <td>A society lady engineers a marriage between her lover and a cabaret dancer who is essentially a prostitute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8474</th>\n",
       "      <td>醜聞</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}]</td>\n",
       "      <td>Akira Kurosawa directed this drama about a paparazzi photo that a tabloid magazine spins into a scandalous story and soon sparks a court case.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      original_title  \\\n",
       "15538                       In a Day   \n",
       "6120   Les dames du Bois de Boulogne   \n",
       "8474                              醜聞   \n",
       "\n",
       "                                                                                              genres  \\\n",
       "15538  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]   \n",
       "6120                                 [{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]   \n",
       "8474                                                                   [{'id': 18, 'name': 'Drama'}]   \n",
       "\n",
       "                                                                                                                                             sentence  \n",
       "15538                                                                             A young woman spends a curiously unpredictable day with a stranger.  \n",
       "6120                                      A society lady engineers a marriage between her lover and a cabaret dancer who is essentially a prostitute.  \n",
       "8474   Akira Kurosawa directed this drama about a paparazzi photo that a tabloid magazine spins into a scandalous story and soon sparks a court case.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SPACY prediction on given input sentence\n",
    "def predict_spacy(model, query_sentence, embed_mat, topk):\n",
    "    \"\"\"\n",
    "    Predict the topk sentences using the SPACY model\n",
    "    \"\"\"\n",
    "    # this creates a Spacy.DOC object\n",
    "    query_embed = model(query_sentence)\n",
    "    \n",
    "    # this calculates the similarity of the input query_sentence\n",
    "    # with very row of the embedded mat\n",
    "    # OUTPUT - this creates a list of scores for the lines\n",
    "    mat = np.array([query_embed.similarity(line) for line in embed_mat])\n",
    "    \n",
    "    # keep if vector has a norm\n",
    "    mat_mask = np.array([True if line.vector_norm else False for line in embed_mat])\n",
    "    best_index = extract_best_indices(mat, topk=topk, mask=mat_mask)\n",
    "    return best_index\n",
    "\n",
    "# run predictions on the model\n",
    "print(\"query sentence = \" + query_sentence)\n",
    "best_index = predict_spacy(nlp, query_sentence, embed_mat, 3)\n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWord2Vec Module from the GenSim package\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Word2Vec Module from the GenSim package\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12936325, 13110000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# create model\n",
    "word2vec_model = Word2Vec(min_count=0, workers=8, vector_size=300)\n",
    "\n",
    "# prepare the vocab\n",
    "word2vec_model.build_vocab(df.tok_lem_sentence.values)\n",
    "\n",
    "# train the model\n",
    "word2vec_model.train(df.tok_lem_sentence.values, total_examples=word2vec_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11048</th>\n",
       "      <td>Marked Woman</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]</td>\n",
       "      <td>Set in the underworld of Manhattan, Marked Woman tells the story of a woman who dares to stand up to one of the city's most powerful gangsters. The women of the story are \"hostesses\". What is implied, but not stated clearly is that they are prostitutes, who work in a gambling den in the city.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9232</th>\n",
       "      <td>苏州河</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 10769, 'name': 'Foreign'}, {'id': 10749, 'name': 'Romance'}]</td>\n",
       "      <td>A tragic love story set in contemporary Shanghai. The film stars Zhou Xun in a dual role as two different women and Jia Hongsheng as a man obsessed with finding a woman from his past.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>L'Année dernière à Marienbad</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]</td>\n",
       "      <td>Takes place in a chateau, an ambiguous story of a man and a woman who may or may not have met last year at Marienbad.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     original_title  \\\n",
       "11048                  Marked Woman   \n",
       "9232                            苏州河   \n",
       "6916   L'Année dernière à Marienbad   \n",
       "\n",
       "                                                                                                                         genres  \\\n",
       "11048  [{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]   \n",
       "9232                          [{'id': 18, 'name': 'Drama'}, {'id': 10769, 'name': 'Foreign'}, {'id': 10749, 'name': 'Romance'}]   \n",
       "6916                                                            [{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    sentence  \n",
       "11048  Set in the underworld of Manhattan, Marked Woman tells the story of a woman who dares to stand up to one of the city's most powerful gangsters. The women of the story are \"hostesses\". What is implied, but not stated clearly is that they are prostitutes, who work in a gambling den in the city.  \n",
       "9232                                                                                                                 A tragic love story set in contemporary Shanghai. The film stars Zhou Xun in a dual role as two different women and Jia Hongsheng as a man obsessed with finding a woman from his past.  \n",
       "6916                                                                                                                                                                                   Takes place in a chateau, an ambiguous story of a man and a woman who may or may not have met last year at Marienbad.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# word2vec prediction\n",
    "def is_word_in_model(word, model):\n",
    "    \"\"\"\n",
    "    Check on individual words '''word''' that exists in '''model'''\n",
    "    \"\"\"\n",
    "    assert type(model).__name__ == 'KeyedVectors'\n",
    "    is_in_vocab = word in model.key_to_index.keys()\n",
    "    return is_in_vocab\n",
    "\n",
    "def predict_w2v(query_sentence, dataset, model, topk=3):\n",
    "    \"\"\"\n",
    "    Predict word to vector using dataset, sentence and model\n",
    "    \"\"\"\n",
    "    query_sentence = query_sentence.split()\n",
    "    in_vocab_list, best_index = [], [0]*topk\n",
    "    for w in query_sentence:\n",
    "        # remove unseen words from query sentence\n",
    "        if is_word_in_model(w, model.wv):\n",
    "            in_vocab_list.append(w)\n",
    "            \n",
    "    # retrieve similarity between two words as a distance\n",
    "    if len(in_vocab_list) > 0:\n",
    "        sim_mat = np.zeros(len(dataset)) \n",
    "        for i, data_sentence in enumerate(dataset):\n",
    "            if data_sentence:\n",
    "                sim_sentence = model.wv.n_similarity(\n",
    "                    in_vocab_list, data_sentence)\n",
    "            else:\n",
    "                sim_sentence = 0\n",
    "            sim_mat[i] = np.array(sim_sentence)\n",
    "            \n",
    "        # take the 5 highest norms\n",
    "        best_index = np.argsort(sim_mat)[::-1][:topk]\n",
    "        \n",
    "    return best_index\n",
    "\n",
    "# run the prediction on query sentence\n",
    "best_index = predict_w2v(query_sentence, df['tok_lem_sentence'].values, word2vec_model)\n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7144e8631704822b8e19356fdd1a514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)001fa/.gitattributes:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7194e7f803ab41698dd34cf1d6c5c9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce38f35079a4748b80186bde7a585e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)3bbb8001fa/README.md:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56faeaf204e94a2480aa0cc0e8eff433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)bb8001fa/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06867dff5b9f40f5806e7d5450ccbb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7a53d0e79642c0b38e55a516afe531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e9bb5cd4544128d31ab8880ff445a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e937e1f73db2400c9f1e2ab92a2e9607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b5d263c0b647eba4446f3a73dcf7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)001fa/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668a803296af4d3c8c4f5ec14c7fde1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c226a284094c7284048aa0c5ab4214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)3bbb8001fa/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa183057e54d48298b2bc3c4f3f4e3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b8001fa/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sentence Transformers using phrases\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load pretrained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode(df.sentence.values, convert_to_tensor=True)\n",
    "query_embedding = model.encode(query_sentence, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: a crime story with a beautiful woman\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "original_title                                                                                                                         Miss Bala\n",
       "genres                                                                               [{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}]\n",
       "sentence          The story of a young woman clinging on to her dream to become a beauty contest queen in a Mexico dominated by organized crime.\n",
       "Name: 18224, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "original_title                                                                                                                                                                                                                                                                                                                                 In the Cut\n",
       "genres                                                                                                                                                                                                                                                                                  [{'id': 9648, 'name': 'Mystery'}, {'id': 53, 'name': 'Thriller'}]\n",
       "sentence          Following the gruesome murder of a young woman in her neighborhood, a self-determined woman living in New York City--as if to test the limits of her own safety--propels herself into an impossibly risky sexual liaison. Soon she grows increasingly wary about the motives of every man with whom she has contact--and about her own.\n",
       "Name: 6736, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "original_title                                                                                    The World of Suzie Wong\n",
       "genres                                                    [{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\n",
       "sentence          Story of the love between a struggling American artist and a beautiful Chinese prostitute in Hong Kong.\n",
       "Name: 8018, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run predictions using the pretrained model and encoding\n",
    "import torch\n",
    "\n",
    "# use cosine similarity and torch to find best scores\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "top_results = torch.topk(cos_scores, k=3)\n",
    "\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"Query:\", query_sentence)\n",
    "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    score = score.cpu().data.numpy()\n",
    "    idx = idx.cpu().data.numpy()\n",
    "    display(df[['original_title', 'genres', 'sentence']].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BERT Text Classification\n",
    "\"\"\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "BERT_BATCH_SIZE = 4\n",
    "MODEL_NAME = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "class BertModel:\n",
    "    def __init__(self, model_name, device=-1, small_memory=True, batch_size=BERT_BATCH_SIZE):\n",
    "        self.model_name = model_name\n",
    "        self._set_device(device)\n",
    "        self.small_device = 'cpu' if small_memory else self.device\n",
    "        self.batch_size = batch_size\n",
    "        self.load_pretrained_model()\n",
    "        \n",
    "    def _set_device(self, device):\n",
    "        if device == -1 or device == 'cpu':\n",
    "            self.device = 'cpu'\n",
    "        elif device == 'cuda' or device == 'gpu':\n",
    "            self.device = 'cuda'\n",
    "        elif isinstance(device, int) or isinstance(device, float):\n",
    "            self.device = 'cuda'\n",
    "        else:  # default\n",
    "            self.device = torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "    def load_pretrained_model(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        device = -1 if self.device == 'cpu' else 0\n",
    "        self.pipeline = pipeline('feature-extraction',\n",
    "                                model=self.model, tokenizer=self.tokenizer, device=device)\n",
    "        \n",
    "    def embed(self, data):\n",
    "        \"\"\"\n",
    "        Create embedded matrix from original sentences\n",
    "        \"\"\"\n",
    "        nb_batches = 1 if (len(data) < self.batch_size) else len(data) // self.batch_size\n",
    "        batches = np.array_split(data, nb_batches)\n",
    "        mean_pooled = []\n",
    "        for batch in tqdm(batches, total=len(batches), desc='Training...'):\n",
    "            mean_pooled.append(self.transform(batch))\n",
    "        mean_pooled_tensor = torch.tensor(len(data), dtype=float).to(self.small_device)\n",
    "        mean_pooled = torch.cat(mean_pooled, out=mean_pooled_tensor)\n",
    "        self.embed_mat = mean_pooled\n",
    "        \n",
    "    @staticmethod\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unqueeze(-1).expand(\n",
    "            token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / \n",
    "            torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
